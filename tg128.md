# tg128 Decode Benchmark ‚Äî Process Diagram

## diagrmm
–ù–∏–∂–µ –¥–ª—è —Ç–µ—Å—Ç–∞ **tg128** ‚Äî —ç—Ç–æ —á–∏—Å—Ç—ã–π decode –Ω–∞ 128 —Ç–æ–∫–µ–Ω–æ–≤ –ø—Ä–∏ –≤–∫–ª—é—á–µ–Ω–Ω–æ–º KV –∫—ç—à–µ.

```mermaid
graph TD;

Load --> Prep --> Prefill --> Warmup --> Run --> Compute --> Report

subgraph Load[load model and deps]
    HF[hf model and tokenizer] --> Init[init model dtype and attention]
    Init --> Dev[move to device]
end

subgraph Prep[prepare start prompt]
    StartTok[set BOS token] --> X0[shape Bx1 single token]
    X0 --> Attn0[attention mask ones]
end

subgraph Prefill[prefill build kv cache]
    PFwd[forward on X0 use_cache true] --> Past[past key values ready]
    PFwd --> LastTok[pick next token]
end

subgraph Warmup[warmup decode passes]
    WLoop[repeat N times] --> WStep[decode one token with past and position ids]
    WStep --> UpdatePast[update kv cache and last token]
end

subgraph Run[timed decode 128 tokens]
    SyncStart[sync device] --> T0[t0 now]
    T0 --> Loop[for step in 1..128]
    Loop --> Pos[set position ids from current length]
    Pos --> StepFwd[forward on last token with past]
    StepFwd --> Update[update kv cache and last token]
    Update --> Loop
    Loop --> SyncEnd[sync device]
end

subgraph Compute[aggregate]
    Times[repeat iters collect durations] --> Median[median time]
    Median --> TPS[tokens per sec equals Bx128 divided by median]
end

subgraph Report[report]
    Row[print table row tg128]
end

Dev --> X0
Attn0 --> Prefill
Warmup --> Run
Run --> Compute
Compute --> Report
```

### –õ–µ–≥–µ–Ω–¥–∞
- **hf model and tokenizer** ‚Äî –∑–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏ –∏ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–∞ –∏–∑ ü§ó Transformers.
- **init model dtype and attention** ‚Äî –≤—ã–±–æ—Ä —Ç–æ—á–Ω–æ—Å—Ç–∏ –∏ —Ä–µ–∞–ª–∏–∑–∞—Ü–∏–∏ –≤–Ω–∏–º–∞–Ω–∏—è sdpa –∏–ª–∏ flash attention 2 –µ—Å–ª–∏ –¥–æ—Å—Ç—É–ø–Ω–æ.
- **move to device** ‚Äî –ø–µ—Ä–µ–Ω–æ—Å –≤–µ—Å–æ–≤ –Ω–∞ GPU ROCm –∏–ª–∏ CUDA.
- **prepare start prompt** ‚Äî —Ñ–æ—Ä–º–∏—Ä—É–µ–º –Ω–∞—á–∞–ª—å–Ω—ã–π –≤—Ö–æ–¥ Bx1 –æ–±—ã—á–Ω–æ —Ç–æ–∫–µ–Ω BOS –∏ –º–∞—Å–∫–∞ –∏–∑ –µ–¥–∏–Ω–∏—Ü.
- **prefill build kv cache** ‚Äî –≤—ã–ø–æ–ª–Ω—è–µ–º –æ–¥–∏–Ω forward —á—Ç–æ–±—ã –∑–∞–ø–æ–ª–Ω–∏—Ç—å kv –∫—ç—à –∏ –ø–æ–ª—É—á–∏—Ç—å –ø–µ—Ä–≤—ã–π —Å–ª–µ–¥—É—é—â–∏–π —Ç–æ–∫–µ–Ω.
- **warmup decode passes** ‚Äî –Ω–µ—Å–∫–æ–ª—å–∫–æ –¥–µ–∫–æ–¥ —à–∞–≥–æ–≤ –±–µ–∑ —Ç–∞–π–º–∏–Ω–≥–∞ –¥–ª—è –ø—Ä–æ–≥—Ä–µ–≤–∞ –ø–æ –æ–¥–Ω–æ–º—É —Ç–æ–∫–µ–Ω—É.
- **timed decode 128 tokens** ‚Äî –∏–∑–º–µ—Ä—è–µ–º —Ü–∏–∫–ª –∏–∑ 128 –¥–µ–∫–æ–¥ —à–∞–≥–æ–≤ —Å past key values –∏ position ids –±–µ–∑ –ø–µ—Ä–µ—Å—á–µ—Ç–∞ –ø–æ–ª–Ω–æ–π –º–∞—Å–∫–∏.
- **aggregate** ‚Äî –ø–æ–≤—Ç–æ—Ä—è–µ–º –Ω–µ—Å–∫–æ–ª—å–∫–æ –∏—Ç–µ—Ä–∞—Ü–∏–π –±–µ—Ä–µ–º –º–µ–¥–∏–∞–Ω—É –∏ —Å—á–∏—Ç–∞–µ–º tokens per sec –∫–∞–∫ Bx128 –¥–µ–ª–∏—Ç—å –Ω–∞ –º–µ–¥–∏–∞–Ω—É.
- **report** ‚Äî –ø–µ—á–∞—Ç–∞–µ–º —Å—Ç—Ä–æ–∫—É —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞ –≤ —Å—Ç–∏–ª–µ llama bench tg128 –∏ t s.

## Use cases

```bash
# default
python bench.py -m ./mistral --tests tg128 --dtype fp16 --batch 1 --attn sdpa --warmup 3 --iters 10

# Eager ‚Äî –∏–Ω–æ–≥–¥–∞ —Å—Ç–∞–±–∏–ª—å–Ω–µ–µ/–±—ã—Å—Ç—Ä–µ–µ –Ω–∞ ROCm –¥–ª—è single-token decode
python bench.py -m ./mistral --tests tg128 --dtype fp16 --batch 1 --attn eager --iters 10

# FlashAttention-2 (–µ—Å–ª–∏ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω –ø–æ–¥ ROCm)
python bench.py -m ./mistral --tests tg128 --dtype fp16 --batch 1 --attn flash_attention_2 --iters 10 

# –£–º–µ—Ä–µ–Ω–Ω–æ–µ —É–≤–µ–ª–∏—á–µ–Ω–∏–µ batch (—Å–ª–µ–¥–∏ –∑–∞ –ø–∞–º—è—Ç—å—é –∏–∑-–∑–∞ KV)
python bench.py -m ./mistral --tests tg128 --dtype fp16 --batch 2 --attn sdpa --iters 8

# 4-–±–∏—Ç –¥–ª—è decode (–µ—Å–ª–∏ bnb –¥–æ—Å—Ç—É–ø–µ–Ω)
python bench.py -m mistralai/Mistral-7B-v0.1 --tests tg128 --quant 4bit --dtype fp16 --batch 1 --attn sdpa --iters 10



```
