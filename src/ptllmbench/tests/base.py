"""
tests/base.py — общие структуры и утилиты для бенчмарков pp*/tg*.

Здесь нет логики модели — только:
- TestResult: результаты одного теста (скорость и статистика времени)
- make_attention_mask: простая маска внимания без паддинга
- replace_special_ids: «очистка» случайных токенов от специальных ID + установка BOS в начале
"""

from dataclasses import dataclass
import torch

@dataclass
class TestResult:
    """
    Результат одного бенч-запуска.

    tps   — tokens per second (токенов в секунду) для данного теста.
    t_med — медианное время одной итерации (сек).
    t_mean— среднее время одной итерации (сек).
    t_std — стандартное отклонение времени (сек).
    """
    tps: float
    t_med: float
    t_mean: float
    t_std: float


def make_attention_mask(x: torch.Tensor) -> torch.Tensor:
    """
    Создаёт маску внимания такого же размера, как input_ids `x`,
    целиком из единиц.

    Почему единицы:
    - В наших бенчах нет паддинга внутри батча (все последовательности одинаковой длины).
    - Каузальность (треугольная маска) обеспечивается самой моделью;
      нам нужна лишь «маска присутствия» токенов, и она везде True/1.

    Параметры:
      x: (batch, seq_len) тензор токенов.

    Возвращает:
      (batch, seq_len) тензор из единиц (dtype и device берутся «как у x»).
    """
    return torch.ones_like(x)


def replace_special_ids(x: torch.Tensor, tok) -> None:
    """
    Заменяет в `x` все «специальные» токены (CLS/SEP/PAD/BOS/EOS и т.д.) на случайные «обычные»
    и принудительно ставит BOS в нулевой позиции каждой строки.

    Зачем:
    - В pp-тестах мы генерируем случайные токены. Если оставить спец-ID, часть моделей
      может вести себя иначе (например, EOS обрежет последовательность).
    - Для консистентности первых токенов ставим BOS в колонку 0.

    Параметры:
      x  : тензор формы (batch, seq_len) на нужном устройстве (CUDA/ROCm/CPU).
      tok: токенизатор HF или совместимый объект с полями:
           - all_special_ids: список специальных ID
           - bos_token_id   : ID BOS (или None)
           - vocab_size     : размер словаря

    Побочный эффект:
      Функция МОДИФИЦИРУЕТ `x` IN-PLACE.

    Примечания по реализации:
    - Делаем булеву маску позиций, где выпали специальные токены, и «перевыбираем» их
      из равномерного распределения [0, vocab_size).
    - Перевыбор в цикле while: на практике одна итерация почти всегда достаточна,
      но если внезапно снова попали на спец-ID, делаем ещё попытку.
    - В конце ставим BOS в нулевую колонку, если у токенизатора он определён.
    """
    # Преобразуем набор спец-ID в тензор на том же устройстве, что и x
    special = set(tok.all_special_ids or [])
    if special:
        spec = torch.tensor(list(special), device=x.device)

        # Находим позиции со спец-ID
        mask = torch.isin(x, spec)

        # Перевыбираем до тех пор, пока все спец-ID не исчезнут
        while bool(mask.any().item()):
            # Сколько позиций нужно заменить сейчас
            n = int(mask.sum().item())
            # Сюда кладём новые случайные ID
            x[mask] = torch.randint(0, tok.vocab_size, (n,), device=x.device)
            # Проверяем, не попали ли снова в спец-ID
            mask = torch.isin(x, spec)

    # Принудительно ставим BOS первым токеном каждой последовательности (если он есть)
    if tok.bos_token_id is not None:
        x[:, 0] = tok.bos_token_id
